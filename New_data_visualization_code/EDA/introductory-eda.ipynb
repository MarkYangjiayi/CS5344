{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-03-30T06:43:15.883677Z","iopub.execute_input":"2022-03-30T06:43:15.883997Z","iopub.status.idle":"2022-03-30T06:43:15.904409Z","shell.execute_reply.started":"2022-03-30T06:43:15.883959Z","shell.execute_reply":"2022-03-30T06:43:15.903524Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Loading Libraries**","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0"}},{"cell_type":"code","source":"import re # for regular expressions\nimport pandas as pd \npd.set_option(\"display.max_colwidth\", 200)\nimport numpy as np \nimport matplotlib.pyplot as plt \nimport seaborn as sns\nimport string\nimport nltk # for text manipulation\nimport warnings \nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n\n%matplotlib inline","metadata":{"execution":{"iopub.status.busy":"2022-03-30T06:43:15.910871Z","iopub.execute_input":"2022-03-30T06:43:15.911308Z","iopub.status.idle":"2022-03-30T06:43:15.922735Z","shell.execute_reply.started":"2022-03-30T06:43:15.911237Z","shell.execute_reply":"2022-03-30T06:43:15.921817Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\nLet's read train and test datasets","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(\"../input/lastdata/lastdata/covid19_vaccine1_01032021_01072021.csv\")\n# df = pd.read_csv(\"../input/newest-data/newest-data/new1/covid19_vaccine2_01122021_01032022.csv\")\n# df = pd.read_csv(\"../input/newest-data/newest-data/new1/sinovac_01032021_01072021.csv\")\n# df = pd.read_csv(\"../input/newest-data/newest-data/new1/sinovac2_01122021_01032022.csv\")\n# df = pd.read_csv(\"../input/newest-data/newest-data/new1/Pfizer_BioNTech_01032021_01072021.csv\",low_memory=False)\n# df = pd.read_csv(\"../input/newest-data/newest-data/new1/Pfizer_BioNTech2_01122021_01032022.csv\",lineterminator='\\n')","metadata":{"execution":{"iopub.status.busy":"2022-03-30T06:43:15.925523Z","iopub.execute_input":"2022-03-30T06:43:15.926314Z","iopub.status.idle":"2022-03-30T06:43:17.187596Z","shell.execute_reply.started":"2022-03-30T06:43:15.926225Z","shell.execute_reply":"2022-03-30T06:43:17.186511Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Text PreProcessing and Cleaning**\n### **Data Inspection** \n\nLet's check out  tweets.","metadata":{}},{"cell_type":"code","source":"print('Dataset size:',df.shape)\nprint('Columns are:',df.columns)","metadata":{"execution":{"iopub.status.busy":"2022-03-30T06:43:17.191578Z","iopub.execute_input":"2022-03-30T06:43:17.192164Z","iopub.status.idle":"2022-03-30T06:43:17.200062Z","shell.execute_reply.started":"2022-03-30T06:43:17.192089Z","shell.execute_reply":"2022-03-30T06:43:17.199134Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The dataset has 89,973 rows and 31 columns.\n\nFrom our case we are only interested in the `tweet`, `hashtags` and `date` columns. We shall drop the rest for now.","metadata":{}},{"cell_type":"code","source":"# use the drop columns function to streamline the dataset\ndf = df.drop(columns=['id', 'time','user_id','username','conversation_id','created_at','timezone', 'name', 'place', 'mentions', 'urls', 'photos', 'replies_count', 'likes_count', 'cashtags', 'link', 'retweet','retweets_count', 'quote_url', 'video', 'near', 'geo', 'source', 'user_rt_id', 'user_rt', 'retweet_id', 'reply_to', 'retweet_date'])\n# df.head(10)","metadata":{"execution":{"iopub.status.busy":"2022-03-30T06:43:17.203138Z","iopub.execute_input":"2022-03-30T06:43:17.203489Z","iopub.status.idle":"2022-03-30T06:43:17.242728Z","shell.execute_reply.started":"2022-03-30T06:43:17.203429Z","shell.execute_reply":"2022-03-30T06:43:17.241613Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#convert data to datetime and strings for manipulation.\ndf[\"tweet\"]= df[\"tweet\"].astype(str)\ndf['date']= pd.to_datetime(df['date'], infer_datetime_format=True)\ndf.info()","metadata":{"execution":{"iopub.status.busy":"2022-03-30T06:43:17.245519Z","iopub.execute_input":"2022-03-30T06:43:17.245881Z","iopub.status.idle":"2022-03-30T06:43:17.358532Z","shell.execute_reply.started":"2022-03-30T06:43:17.245818Z","shell.execute_reply":"2022-03-30T06:43:17.357359Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### We will only work on a sample of the dataset to make the execeution run quicker (10% Of the total dataset)","metadata":{}},{"cell_type":"code","source":"df = df.sample(frac=.1, random_state=1111)","metadata":{"execution":{"iopub.status.busy":"2022-03-30T06:43:17.36037Z","iopub.execute_input":"2022-03-30T06:43:17.360667Z","iopub.status.idle":"2022-03-30T06:43:17.384303Z","shell.execute_reply.started":"2022-03-30T06:43:17.360618Z","shell.execute_reply":"2022-03-30T06:43:17.383125Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Pre-processing\n\nThe preprocessing of the text data is an essential step as it makes the raw text ready for mining, i.e., it becomes easier to extract information from the text and apply machine learning algorithms to it. If we skip this step then there is a higher chance that you are working with noisy and inconsistent data. The objective of this step is to clean noise those are less relevant to find the sentiment of tweets such as punctuation, special characters, numbers, and terms which don’t carry much weightage in context to the text.\n\n### Characteristic features of Tweets \n\nFrom the perspective of Sentiment\nAnalysis, we discuss a few characteristics of Twitter:\n\n**Length of a Tweet**\n     The maximum length of a Twitter message is 140 characters. This means that we can practically consider a tweet to be a single sentence, void of complex grammatical constructs. This is a vast difference from traditional subjects of Sentiment Analysis, such as movie reviews. \n     \n**Language used**\n     Twitter is used via a variety of media including SMS and mobile phone apps. Because of this and the 140-character limit, language used in Tweets tend be more colloquial, and filled with slang and misspellings. Use of hashtags also gained popularity on Twitter and is a primary feature in any given tweet. \n     \n**Data availability**\n     Another difference is the magnitude of data available. With the Twitter API, it is easy to collect millions of tweets for training. There also exist a few datasets that have automatically and manually labelled the tweets. \n     \n**Domain of topics**\n     People often post about their likes and dislikes on social media. These are not all concentrated around one topic. ","metadata":{}},{"cell_type":"markdown","source":"### Cleaning The Data\nWhen dealing with numerical data, data cleaning often involves removing null values and duplicate data, dealing with outliers, etc. With text data, there are some common data cleaning techniques, which are also known as text pre-processing techniques.\n\nWith text data, this cleaning process can go on forever. There's always an exception to every cleaning step. So, we're going to follow the MVP (minimum viable product) approach - start simple and iterate. Here are a bunch of things you can do to clean your data. We're going to execute just the common cleaning steps here and the rest can be done at a later point to improve our results.\n\n### Common data cleaning steps on all text:\n\nMake text all lower case\nRemove punctuation\nRemove numerical values\nRemove common non-sensical text (/n)\nTokenize text\nRemove stop words\nMore data cleaning steps after tokenization:\n\nStemming / lemmatization\nParts of speech tagging\nCreate bi-grams or tri-grams\nDeal with typos\n\n### Specific Tweet oriented cleaning using the  tweet-preprocessor module\n\n### A) Removing Twitter Handles (@user)\n\nAs mentioned above, the tweets contain lots of twitter handles (@user), that is how a Twitter user acknowledged on Twitter. We will remove all these twitter handles from the data as they don’t convey much information.\n\n### B) Removing Punctuations,Links, Numbers, and Special Characters\n\nAs discussed, punctuations, numbers and special characters do not help much. It is better to remove them from the text just as we removed the twitter handles.\n\n### C) Tokenization\nTokens are individual terms or words, and tokenization is the process of splitting a string of text into tokens.\n\n### D) Stemming\nStemming is a rule-based process of stripping the suffixes (“ing”, “ly”, “es”, “s” etc) from a word. For example, For example – “play”, “player”, “played”, “plays” and “playing” are the different variations of the word – “play”.\n","metadata":{}},{"cell_type":"code","source":"import string\n\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk import re\n\nMIN_YEAR = 1900\nMAX_YEAR = 2100\n\n\ndef get_url_patern():\n    return re.compile(\n        r'(https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|https?:\\/\\/(?:www\\.|(?!www))'\n        r'[a-zA-Z0-9]\\.[^\\s]{2,}|www\\.[a-zA-Z0-9]\\.[^\\s]{2,})')\n\n\ndef get_emojis_pattern():\n    try:\n        # UCS-4\n        emojis_pattern = re.compile(u'([\\U00002600-\\U000027BF])|([\\U0001f300-\\U0001f64F])|([\\U0001f680-\\U0001f6FF])')\n    except re.error:\n        # UCS-2\n        emojis_pattern = re.compile(\n            u'([\\u2600-\\u27BF])|([\\uD83C][\\uDF00-\\uDFFF])|([\\uD83D][\\uDC00-\\uDE4F])|([\\uD83D][\\uDE80-\\uDEFF])')\n    return emojis_pattern\n\n\ndef get_hashtags_pattern():\n    return re.compile(r'#\\w*')\n\n\ndef get_single_letter_words_pattern():\n    return re.compile(r'(?<![\\w\\-])\\w(?![\\w\\-])')\n\n\ndef get_blank_spaces_pattern():\n    return re.compile(r'\\s{2,}|\\t')\n\n\ndef get_twitter_reserved_words_pattern():\n    return re.compile(r'(RT|rt|FAV|fav|VIA|via)')\n\n\ndef get_mentions_pattern():\n    return re.compile(r'@\\w*')\n\n\ndef is_year(text):\n    if (len(text) == 3 or len(text) == 4) and (MIN_YEAR < len(text) < MAX_YEAR):\n        return True\n    else:\n        return False\n\n\nclass TwitterPreprocessor:\n\n    def __init__(self, text: str):\n        self.text = text\n\n    def fully_preprocess(self):\n        return self \\\n            .remove_urls() \\\n            .remove_mentions() \\\n            .remove_hashtags() \\\n            .remove_twitter_reserved_words() \\\n            .remove_punctuation() \\\n            .remove_single_letter_words() \\\n            .remove_blank_spaces() \\\n            .remove_stopwords() \\\n            .remove_numbers()\n\n    def remove_urls(self):\n        self.text = re.sub(pattern=get_url_patern(), repl='', string=self.text)\n        return self\n\n    def remove_punctuation(self):\n        self.text = self.text.translate(str.maketrans('', '', string.punctuation))\n        return self\n\n    def remove_mentions(self):\n        self.text = re.sub(pattern=get_mentions_pattern(), repl='', string=self.text)\n        return self\n\n    def remove_hashtags(self):\n        self.text = re.sub(pattern=get_hashtags_pattern(), repl='', string=self.text)\n        return self\n\n    def remove_twitter_reserved_words(self):\n        self.text = re.sub(pattern=get_twitter_reserved_words_pattern(), repl='', string=self.text)\n        return self\n\n    def remove_single_letter_words(self):\n        self.text = re.sub(pattern=get_single_letter_words_pattern(), repl='', string=self.text)\n        return self\n\n    def remove_blank_spaces(self):\n        self.text = re.sub(pattern=get_blank_spaces_pattern(), repl=' ', string=self.text)\n        return self\n\n    def remove_stopwords(self, extra_stopwords=None):\n        if extra_stopwords is None:\n            extra_stopwords = []\n        text = nltk.word_tokenize(self.text)\n        stop_words = set(stopwords.words('english'))\n\n        new_sentence = []\n        for w in text:\n            if w not in stop_words and w not in extra_stopwords:\n                new_sentence.append(w)\n        self.text = ' '.join(new_sentence)\n        return self\n\n    def remove_numbers(self, preserve_years=False):\n        text_list = self.text.split(' ')\n        for text in text_list:\n            if text.isnumeric():\n                if preserve_years:\n                    if not is_year(text):\n                        text_list.remove(text)\n                else:\n                    text_list.remove(text)\n\n        self.text = ' '.join(text_list)\n        return self\n\n    def lowercase(self):\n        self.text = self.text.lower()\n        return self","metadata":{"execution":{"iopub.status.busy":"2022-03-30T06:43:17.386068Z","iopub.execute_input":"2022-03-30T06:43:17.386572Z","iopub.status.idle":"2022-03-30T06:43:17.663766Z","shell.execute_reply.started":"2022-03-30T06:43:17.386391Z","shell.execute_reply":"2022-03-30T06:43:17.66234Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Clean tweets and append to new column\ntweets = df['tweet']\nclean_tweets = []\nfor tweet in tweets:\n    c = TwitterPreprocessor((tweet))\n    c.fully_preprocess()\n    c = c.text\n    clean_tweets.append(c)\n    \ndf['clean_tweets'] = clean_tweets \n# df.head(5)","metadata":{"execution":{"iopub.status.busy":"2022-03-30T06:43:17.666253Z","iopub.execute_input":"2022-03-30T06:43:17.666885Z","iopub.status.idle":"2022-03-30T06:43:23.163195Z","shell.execute_reply.started":"2022-03-30T06:43:17.666806Z","shell.execute_reply":"2022-03-30T06:43:23.161864Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Visualization from Tweets**\n### **A) Understanding the common words used in the tweets: WordCloud**\n\nNow we want to understand the common words by plotting wordclouds.\n\nA wordcloud is a visualization wherein the most frequent words appear in large size and the less frequent words appear in smaller sizes.\n\nLet’s visualize all the words our data using the wordcloud plot.","metadata":{}},{"cell_type":"code","source":"\nall_words = ' '.join([text for text in df['clean_tweets']])\nfrom wordcloud import WordCloud\nfrom imageio import imread\nmk=imread(\"/kaggle/input/aimagedata/e.png\")\nwordcloud = WordCloud(mask=mk, background_color = 'white', width=1000, height=600, random_state=25, max_font_size=200).generate(all_words)\n\nplt.figure(figsize=(15, 10))\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis('off')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-30T06:43:23.16507Z","iopub.execute_input":"2022-03-30T06:43:23.165568Z","iopub.status.idle":"2022-03-30T06:43:28.01298Z","shell.execute_reply.started":"2022-03-30T06:43:23.165442Z","shell.execute_reply":"2022-03-30T06:43:28.01158Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Topic Model Analysis**\n\nTopic Modeling (TM) consists of finding the information contained in textual documents (information retrieval in English) and presenting it in the form of themes (depending on the technique used, the relative importance of the themes can also be found ).\n\nTM is therefore an unsupervised technique for classifying documents in multiple themes (Unsupervised Learning in English).\n\nFrom the point of view of the representation space, the TM is a reduction of dimensions in the vector representation of a document : instead of representing a document of a corpus by a vector in the space of the words composing the vocabulary of this corpus is represented by a vector in the space of the themes of this corpus , each value of this vector corresponding to the relative importance of the theme in this document. \n","metadata":{}},{"cell_type":"code","source":"import sys\n# !{sys.executable} -m spacy download en\nimport re, numpy as np, pandas as pd\nfrom pprint import pprint\n\n# Gensim\nimport gensim, spacy, logging, warnings\nimport gensim.corpora as corpora\nfrom gensim.utils import lemmatize, simple_preprocess\nfrom gensim.models import CoherenceModel\n\n# NLTK Stop words\nfrom nltk.corpus import stopwords\nstop_words = stopwords.words('english')\nstop_words.extend(['from', 'subject', 're', 'edu', 'use', 'not', 'would', 'say', 'could', '_', 'be', 'know', 'good', 'go', 'get', 'do', 'done', 'try', 'many', 'some', 'nice', 'thank', 'think', 'see', 'rather', 'easy', 'easily', 'lot', 'lack', 'make', 'want', 'seem', 'run', 'need', 'even', 'right', 'line', 'even', 'also', 'may', 'take', 'come'])\n","metadata":{"execution":{"iopub.status.busy":"2022-03-30T06:43:28.014887Z","iopub.execute_input":"2022-03-30T06:43:28.01527Z","iopub.status.idle":"2022-03-30T06:43:28.027177Z","shell.execute_reply.started":"2022-03-30T06:43:28.015206Z","shell.execute_reply":"2022-03-30T06:43:28.025914Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def sent_to_words(sentences):\n    for sent in sentences:\n        sent = re.sub('\\S*@\\S*\\s?', '', sent) \n        sent = re.sub('\\s+', ' ', sent)  # remove newline chars\n        sent = re.sub(\"\\'\", \"\", sent)  # remove single quotes\n        sent = gensim.utils.simple_preprocess(str(sent), deacc=True) \n        yield(sent)  \n# Convert to list\ndata = df.clean_tweets.values.tolist()\ndata_words = list(sent_to_words(data))\nprint(data_words[:1])","metadata":{"execution":{"iopub.status.busy":"2022-03-30T06:43:28.02856Z","iopub.execute_input":"2022-03-30T06:43:28.028997Z","iopub.status.idle":"2022-03-30T06:43:29.003721Z","shell.execute_reply.started":"2022-03-30T06:43:28.02895Z","shell.execute_reply":"2022-03-30T06:43:29.002215Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Build the bigram and trigram models\nbigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\ntrigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \nbigram_mod = gensim.models.phrases.Phraser(bigram)\ntrigram_mod = gensim.models.phrases.Phraser(trigram)\n\ndef process_words(texts, stop_words=stop_words, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n    \"\"\"Remove Stopwords, Form Bigrams, Trigrams and Lemmatization\"\"\"\n    texts = [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n    texts = [bigram_mod[doc] for doc in texts]\n    texts = [trigram_mod[bigram_mod[doc]] for doc in texts]\n    texts_out = []\n    nlp = spacy.load('en', disable=['parser', 'ner'])\n    for tweet in texts:\n        doc = nlp(\" \".join(tweet)) \n        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n    # remove stopwords once more after lemmatization\n    texts_out = [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts_out]    \n    return texts_out\n\ndata_ready = process_words(data_words)  # processed Tweet Data!","metadata":{"execution":{"iopub.status.busy":"2022-03-30T06:43:29.005844Z","iopub.execute_input":"2022-03-30T06:43:29.006241Z","iopub.status.idle":"2022-03-30T06:44:10.655632Z","shell.execute_reply.started":"2022-03-30T06:43:29.006182Z","shell.execute_reply":"2022-03-30T06:44:10.654308Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Build the Topic Model**\nTo build the LDA topic model using LdaModel(), you need the corpus and the dictionary. Let’s create them first and then build the model. The trained topics (keywords and weights) are printed below as well.","metadata":{}},{"cell_type":"code","source":"# Create Dictionary\nid2word = corpora.Dictionary(data_ready)\n\n# Create Corpus: Term Document Frequency\ncorpus = [id2word.doc2bow(text) for text in data_ready]\n\n# Build LDA model\nlda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n                                           id2word=id2word,\n                                           num_topics=4, \n                                           random_state=100,\n                                           update_every=1,\n                                           chunksize=10,\n                                           passes=10,\n                                           alpha='symmetric',\n                                           iterations=100,\n                                           per_word_topics=True)\n\n# print(lda_model.print_topics())","metadata":{"execution":{"iopub.status.busy":"2022-03-30T06:44:10.657674Z","iopub.execute_input":"2022-03-30T06:44:10.658183Z","iopub.status.idle":"2022-03-30T06:45:32.092411Z","shell.execute_reply.started":"2022-03-30T06:44:10.6581Z","shell.execute_reply":"2022-03-30T06:45:32.091471Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def format_topics_sentences(ldamodel=None, corpus=corpus, texts=data):\n    # Init output\n    tweet_topics_df = pd.DataFrame()\n\n    # Get main topic in each document\n    for i, row_list in enumerate(ldamodel[corpus]):\n        row = row_list[0] if ldamodel.per_word_topics else row_list            \n        # print(row)\n        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n        # Get the Dominant topic, Perc Contribution and Keywords for each document\n        for j, (topic_num, prop_topic) in enumerate(row):\n            if j == 0:  # => dominant topic\n                wp = ldamodel.show_topic(topic_num)\n                topic_keywords = \", \".join([word for word, prop in wp])\n                tweet_topics_df = tweet_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n            else:\n                break\n    tweet_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n\n    # Add original text to the end of the output\n    contents = pd.Series(texts)\n    tweet_topics_df = pd.concat([tweet_topics_df, contents], axis=1)\n    return(tweet_topics_df)\n\n\ndf_topic_tweet_keywords = format_topics_sentences(ldamodel=lda_model, corpus=corpus, texts=data_ready)\n\n# Format\ndf_dominant_topic = df_topic_tweet_keywords.reset_index()\ndf_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Tweet']\n# df_dominant_topic.head(10)","metadata":{"execution":{"iopub.status.busy":"2022-03-30T06:45:32.093721Z","iopub.execute_input":"2022-03-30T06:45:32.094124Z","iopub.status.idle":"2022-03-30T06:46:14.53949Z","shell.execute_reply.started":"2022-03-30T06:45:32.094083Z","shell.execute_reply":"2022-03-30T06:46:14.538294Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Frequency Distribution of Word Counts in Tweets\nLet’s plot the tweet word counts distribution.","metadata":{}},{"cell_type":"code","source":"doc_lens = [len(d) for d in df_dominant_topic.Tweet]\n\n# Plot\nplt.figure(figsize=(18,7), dpi=55)\nplt.hist(doc_lens, bins = 100, color='black')\n\nplt.gca().set(xlim=(0, 30), ylabel='Tweets Number', xlabel='Word Count')\nplt.tick_params(size=18)\nplt.xticks(np.linspace(0,30,9))\nplt.title('Tweet Word Counts Distribution', fontdict=dict(size=25))\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-30T06:46:14.54113Z","iopub.execute_input":"2022-03-30T06:46:14.541438Z","iopub.status.idle":"2022-03-30T06:46:15.072939Z","shell.execute_reply.started":"2022-03-30T06:46:14.541392Z","shell.execute_reply":"2022-03-30T06:46:15.071629Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.colors as mcolors\ncols = [color for name, color in mcolors.TABLEAU_COLORS.items()]  # more colors: 'mcolors.XKCD_COLORS'\n\nfig, axes = plt.subplots(4,1,figsize=(10,8), dpi=55, sharex=True, sharey=True)\n\nfor i, ax in enumerate(axes.flatten()):    \n    df_dominant_topic_sub = df_dominant_topic.loc[df_dominant_topic.Dominant_Topic == i, :]\n    doc_lens = [len(d) for d in df_dominant_topic_sub.Tweet]\n    ax.hist(doc_lens, bins = 50, color=cols[i])\n    ax.tick_params(axis='y', labelcolor=cols[i], color=cols[i])\n    sns.kdeplot(doc_lens, color=\"black\", shade=False, ax=ax.twinx())\n    ax.set(xlim=(0, 30), xlabel='Tweet Word Count')\n    ax.set_ylabel('Tweets Numbers', color=cols[i])\n    ax.set_title('Topic: '+str(i), fontdict=dict(size=16, color=cols[i]))\n\nfig.tight_layout()\nfig.subplots_adjust(top=0.90)\nplt.xticks(np.linspace(0,30,9))\nfig.suptitle('Distribution of Tweet Word Counts by Dominant Topic', fontsize=24)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-30T06:46:15.075298Z","iopub.execute_input":"2022-03-30T06:46:15.076165Z","iopub.status.idle":"2022-03-30T06:46:17.315489Z","shell.execute_reply.started":"2022-03-30T06:46:15.076065Z","shell.execute_reply":"2022-03-30T06:46:17.31413Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Word Clouds of Top N Keywords in Each Topic\n\nThough we’ve already seen what are the topic keywords in each topic, a word cloud with the size of the words proportional to the weight is a pleasant sight. The coloring of the topics we’ve taken here is followed in the subsequent plots as well.","metadata":{}},{"cell_type":"code","source":"# 1. Wordcloud of Top N words in each topic\nfrom matplotlib import pyplot as plt\nfrom wordcloud import WordCloud, STOPWORDS\nimport matplotlib.colors as mcolors\n\ncols = [color for name, color in mcolors.TABLEAU_COLORS.items()]  # more colors: 'mcolors.XKCD_COLORS'\n\ncloud = WordCloud(stopwords=stop_words,\n                  background_color='white',\n                  width=2500,\n                  height=1800,\n                  max_words=10,\n                  colormap='tab10',\n                  color_func=lambda *args, **kwargs: cols[i],\n                  prefer_horizontal=1.0)\n\ntopics = lda_model.show_topics(formatted=False)\n\nfig, axes = plt.subplots(1, 4, figsize=(15,15), sharex=True, sharey=True)\n\nfor i, ax in enumerate(axes.flatten()):\n    fig.add_subplot(ax)\n    topic_words = dict(topics[i][1])\n    cloud.generate_from_frequencies(topic_words, max_font_size=300)\n    plt.gca().imshow(cloud)\n    plt.gca().set_title('Topic ' + str(i), fontdict=dict(size=15))\n    plt.gca().axis('off')\n\n\nplt.subplots_adjust(wspace=0, hspace=0)\nplt.axis('off')\nplt.margins(x=0, y=0)\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-30T06:46:17.317868Z","iopub.execute_input":"2022-03-30T06:46:17.318762Z","iopub.status.idle":"2022-03-30T06:46:20.1134Z","shell.execute_reply.started":"2022-03-30T06:46:17.318677Z","shell.execute_reply":"2022-03-30T06:46:20.112282Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Word Counts of Topic Keywords\nWhen it comes to the keywords in the topics, the importance (weights) of the keywords matters. Along with that, how frequently the words have appeared in the tweets is also interesting to look.\n\nLet’s plot the word counts and the weights of each keyword in the same chart.\n\nWe want to keep an eye out on the words that occur in multiple topics and the ones whose relative frequency is more than the weight. Often such words turn out to be less important. The chart we’ve drawn below is a result of adding several such words to the stop words list in the beginning and re-running the training process.","metadata":{}},{"cell_type":"code","source":"from collections import Counter\ntopics = lda_model.show_topics(formatted=False)\ndata_flat = [w for w_list in data_ready for w in w_list]\ncounter = Counter(data_flat)\n\nout = []\nfor i, topic in topics:\n    for word, weight in topic:\n        out.append([word, i , weight, counter[word]])\n\ndf_topics = pd.DataFrame(out, columns=['word', 'topic_id', 'importance', 'word_count'])        \n\n# Plot Word Count and Weights of Topic Keywords\nfig, axes = plt.subplots(2, 2, figsize=(10,8), sharey=True, dpi=75)\ncols = [color for name, color in mcolors.XKCD_COLORS.items()]\nfor i, ax in enumerate(axes.flatten()):\n    ax.bar(x='word', height=\"word_count\", data=df_topics.loc[df_topics.topic_id==i, :], color=cols[i], width=0.5, alpha=0.3, label='Word Count')\n    ax_twin = ax.twinx()\n    ax_twin.bar(x='word', height=\"importance\", data=df_topics.loc[df_topics.topic_id==i, :], color=cols[i], width=0.2, label='Weights')\n    ax.set_ylabel('Word Count', color=cols[i])\n    ax_twin.set_ylim(0, 0.050); ax.set_ylim(0, 1000)\n    ax.set_title('Topic: ' + str(i), color=cols[i], fontsize=16)\n    ax.tick_params(axis='y', left=False)\n    ax.set_xticklabels(df_topics.loc[df_topics.topic_id==i, 'word'], rotation=30, horizontalalignment= 'right')\n    ax.legend(loc='upper left'); ax_twin.legend(loc='upper right')\n\nfig.tight_layout(w_pad=2)    \nfig.suptitle('Word Count and Importance of Topic Keywords', fontsize=22, y=1.05)    \nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-30T06:46:20.115201Z","iopub.execute_input":"2022-03-30T06:46:20.11568Z","iopub.status.idle":"2022-03-30T06:46:21.829643Z","shell.execute_reply.started":"2022-03-30T06:46:20.115615Z","shell.execute_reply":"2022-03-30T06:46:21.828285Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## What are the most discussed topics in the documents?\n\nLet’s compute the total number of documents attributed to each topic.","metadata":{}},{"cell_type":"code","source":"# Sentence Coloring of N Sentences\ndef topics_per_document(model, corpus, start=0, end=1):\n    corpus_sel = corpus[start:end]\n    dominant_topics = []\n    topic_percentages = []\n    for i, corp in enumerate(corpus_sel):\n        topic_percs, wordid_topics, wordid_phivalues = model[corp]\n        dominant_topic = sorted(topic_percs, key = lambda x: x[1], reverse=True)[0][0]\n        dominant_topics.append((i, dominant_topic))\n        topic_percentages.append(topic_percs)\n    return(dominant_topics, topic_percentages)\n\ndominant_topics, topic_percentages = topics_per_document(model=lda_model, corpus=corpus, end=-1)            \n\n# Distribution of Dominant Topics in Each Document\ndf_dominant = pd.DataFrame(dominant_topics, columns=['Document_Id', 'Dominant_Topic'])\ndominant_topic_in_each_doc = df_dominant.groupby('Dominant_Topic').size()\ndf_dominant_topic_in_each_doc = dominant_topic_in_each_doc.to_frame(name='count').reset_index()\n\n# Total Topic Distribution by actual weight\ntopic_weightage_by_doc = pd.DataFrame([dict(t) for t in topic_percentages])\ndf_topic_weightage_by_doc = topic_weightage_by_doc.sum().to_frame(name='count').reset_index()\n\n# Top 3 Keywords for each Topic\ntopic_top3words = [(i, topic) for i, topics in lda_model.show_topics(formatted=False) \n                                 for j, (topic, wt) in enumerate(topics) if j < 3]\n\ndf_top3words_stacked = pd.DataFrame(topic_top3words, columns=['topic_id', 'words'])\ndf_top3words = df_top3words_stacked.groupby('topic_id').agg(', \\n'.join)\ndf_top3words.reset_index(level=0,inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-03-30T06:46:21.841292Z","iopub.execute_input":"2022-03-30T06:46:21.842228Z","iopub.status.idle":"2022-03-30T06:46:26.57665Z","shell.execute_reply.started":"2022-03-30T06:46:21.842118Z","shell.execute_reply":"2022-03-30T06:46:26.574937Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from matplotlib.ticker import FuncFormatter\n\n# Plot\nfig, (ax1, ax2) = plt.subplots(2, 1, figsize=(8, 8), dpi=75, sharex=True, sharey=True)\n\n# Topic Distribution by Dominant Topics\nax1.bar(x='Dominant_Topic', height='count', data=df_dominant_topic_in_each_doc, width=.5, color='black')\nax1.set_xticks(range(df_dominant_topic_in_each_doc.Dominant_Topic.unique().__len__()))\ntick_formatter = FuncFormatter(lambda x, pos: 'Topic ' + str(x)+ '\\n' + df_top3words.loc[df_top3words.topic_id==x, 'words'].values[0])\nax1.xaxis.set_major_formatter(tick_formatter)\nax1.set_title('Number of Tweets by Dominant Topic', fontdict=dict(size=10))\nax1.set_ylabel('Tweets Numbera')\nax1.set_ylim(0, 25000)\n\n# Topic Distribution by Topic Weights\nax2.bar(x='index', height='count', data=df_topic_weightage_by_doc, width=.5, color='red')\nax2.set_xticks(range(df_topic_weightage_by_doc.index.unique().__len__()))\nax2.xaxis.set_major_formatter(tick_formatter)\nax2.set_title('Number of Tweets by Topic Weightage', fontdict=dict(size=10))\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-30T06:46:26.578703Z","iopub.execute_input":"2022-03-30T06:46:26.579256Z","iopub.status.idle":"2022-03-30T06:46:27.105105Z","shell.execute_reply.started":"2022-03-30T06:46:26.579107Z","shell.execute_reply":"2022-03-30T06:46:27.103235Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Sentiment Analysis Using VADER**\n\nCreating your own sentiment analysis model from scratch can be very difficult and tedious for a few reason. You need to find relevant data to your problem, create a LOT of labeled data for training, and you must perform data clean up and NLP pre-processing. Luckily for us, VADER is a readily available pre-trained sentiment analysis that thrives on social media data. \nSome of the big advantages include:\n1. Analysis of polarity (positive or negative sentiment) as well as valence (intensity of the sentiment — i.e. ‘excellent’ has a higher intensity than ‘good’).\n2. Handles slang (‘lol’, ‘sux’) and emojis, which are prevalent in tweets\n3. Accounts for capital letters and punctuation (i.e. ‘GOOD!!’ is more positive than ‘good’)\n\nFor more information on VADER you can access the [github repository](https://github.com/cjhutto/vaderSentiment) or the [paper written by the authors](http://comp.social.gatech.edu/papers/icwsm14.vader.hutto.pdf). \nFor other sentiment analysis tools you can check out this [github page.](https://github.com/laugustyniak/awesome-sentiment-analysis).\n\nThe first analysis we are going to do is to plot a histogram of all of the sentiment scores we collected on our tweets. \n","metadata":{}},{"cell_type":"code","source":"import nltk\nnltk.download(\"vader_lexicon\")","metadata":{"execution":{"iopub.status.busy":"2022-03-30T06:46:27.107745Z","iopub.execute_input":"2022-03-30T06:46:27.108664Z","iopub.status.idle":"2022-03-30T06:46:47.163958Z","shell.execute_reply.started":"2022-03-30T06:46:27.108365Z","shell.execute_reply":"2022-03-30T06:46:47.162851Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from nltk.sentiment.vader import SentimentIntensityAnalyzer","metadata":{"execution":{"iopub.status.busy":"2022-03-30T06:46:47.165837Z","iopub.execute_input":"2022-03-30T06:46:47.166529Z","iopub.status.idle":"2022-03-30T06:46:47.190013Z","shell.execute_reply.started":"2022-03-30T06:46:47.16646Z","shell.execute_reply":"2022-03-30T06:46:47.188927Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create analyzer object \nanalyzer = SentimentIntensityAnalyzer()\n\n# get a list of scores and plot\nscores = [analyzer.polarity_scores(tweet)['compound'] for tweet in df['clean_tweets']]\nplt.hist(scores, bins=20)","metadata":{"execution":{"iopub.status.busy":"2022-03-30T06:46:47.191434Z","iopub.execute_input":"2022-03-30T06:46:47.191918Z","iopub.status.idle":"2022-03-30T06:46:50.050415Z","shell.execute_reply.started":"2022-03-30T06:46:47.19187Z","shell.execute_reply":"2022-03-30T06:46:50.048397Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"VADER gives back 4 types of polarity scores for every call: Positive, negative, neutral or compound. \n\nIn our code, we only consider the compound score  which is a combination of the other three plus some additional rules and a normalization between -1 and 1. \n\nOne thing to notice from our histogram is that many tweets have a neutral score, and there are only slightly more negative ones in this sample.\n","metadata":{}},{"cell_type":"code","source":"sentiment = df['clean_tweets'].apply(lambda x: analyzer.polarity_scores(x))\ndf = pd.concat([df,sentiment.apply(pd.Series)],1)\n# df.head(5)","metadata":{"execution":{"iopub.status.busy":"2022-03-30T06:46:50.052793Z","iopub.execute_input":"2022-03-30T06:46:50.053305Z","iopub.status.idle":"2022-03-30T06:46:56.319883Z","shell.execute_reply.started":"2022-03-30T06:46:50.053223Z","shell.execute_reply":"2022-03-30T06:46:56.318544Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Analyzing Sentiment\nFirst let’s just call `df.describe()` and get some basic information on our dataset now .","metadata":{}},{"cell_type":"code","source":"df.describe()","metadata":{"execution":{"iopub.status.busy":"2022-03-30T06:46:56.321944Z","iopub.execute_input":"2022-03-30T06:46:56.322426Z","iopub.status.idle":"2022-03-30T06:46:56.585361Z","shell.execute_reply.started":"2022-03-30T06:46:56.322359Z","shell.execute_reply":"2022-03-30T06:46:56.584465Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Looking at the compound score we can see on average tweets are neutral, with a mean sentiment of .001.\n\nPlotting this data will give us a better idea of what it looks like. Before we plot we make a few changes to the dataframe for ease of use, sorting all the values by timestamp so they’re in order, copying the timestamp to the index to make graphing easier, and calculating an expanding and rolling mean for compound sentiment scores.","metadata":{}},{"cell_type":"code","source":"df.index = pd.to_datetime(df['date'])\ndf = df.sort_index()\ndf['mean'] = df['compound'].expanding().mean()\ndf['rolling'] = df['compound'].rolling('1d').mean()","metadata":{"execution":{"iopub.status.busy":"2022-03-30T06:46:56.586855Z","iopub.execute_input":"2022-03-30T06:46:56.587132Z","iopub.status.idle":"2022-03-30T06:46:56.614025Z","shell.execute_reply.started":"2022-03-30T06:46:56.587079Z","shell.execute_reply":"2022-03-30T06:46:56.61304Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now using matplotlib, with import matplotlib.pyplot as plt, we can create a quick chart of our tweets and their sentiment over time.","metadata":{}},{"cell_type":"code","source":"import datetime as dt\n\nfig = plt.figure(figsize=(20,5))\nax = fig.add_subplot(111)\nax.scatter(df['date'],df['compound'], label='Tweet Sentiment')\nax.plot(df['date'],df['rolling'], color ='r', label='Rolling Mean')\nax.plot(df['date'],df['mean'], color='g', label='Expanding Mean')\n#ax.set_xlim([dt.date(2019,6,15),dt.date(2019,10,15)])\nax.set(title='Vaccination Tweets over Time', xlabel='Date', ylabel='Sentiment')\nax.legend(loc='best')\nfig.tight_layout()\nplt.xticks(\n    rotation=45, \n    horizontalalignment='right',\n    fontweight='light',\n    fontsize='x-large'  \n)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-30T06:46:56.615736Z","iopub.execute_input":"2022-03-30T06:46:56.616024Z","iopub.status.idle":"2022-03-30T06:46:57.781678Z","shell.execute_reply.started":"2022-03-30T06:46:56.615975Z","shell.execute_reply":"2022-03-30T06:46:57.780329Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let’s try to tackle things one at a time here. First let’s look at those tweets with a sentiment of 0. Seborn’s distplot is a quick way to see the distribution of sentiment scores across our tweets.","metadata":{}},{"cell_type":"code","source":"fig = plt.figure(figsize=(15,5))\nax = fig.add_subplot(111)\nax.set(title='Vaccination Tweets Sentiment Score', xlabel='Compund Sentiment Score', ylabel='Frequency')\nsns.distplot(df['compound'], bins=15, ax=ax)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-30T06:46:57.783557Z","iopub.execute_input":"2022-03-30T06:46:57.783854Z","iopub.status.idle":"2022-03-30T06:46:58.142617Z","shell.execute_reply.started":"2022-03-30T06:46:57.783805Z","shell.execute_reply":"2022-03-30T06:46:58.14118Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let’s see if we can get a little bit clearer picture of our sentiment over time. \n\nOverall our data is noisy, there is just too much of it. \nTaking a sample of our data might make it easier to see the trends happening. \nWe’ll use pandas sample() function to retain just a tenth of our 89,973 tweets.","metadata":{}}]}